\chapter{まとめ}
本稿では，容易に導出できる新しい特徴を利用して，圧縮ベースパターン認識手法の改善を行う．
圧縮ベースパターン認識手法は(1) 圧縮後のファイルサイズを利用する手法と (2) 圧縮辞書から特徴抽出する手法に大別される．
本研究では，前者でよく用いられる手法であるPRDCと，後者の最新の研究であるNMDを取り上げ，精度向上を試みた．

本稿では，まずPRDCが使用する圧縮率はテキスト内の単語頻度のみで決定され，異なる単語間の関係を一切利用していない特徴である事を指摘した．そこで，単語の順序関係情報を特徴ベクトルに埋め込んだ，HOPRDCを提案する．具体的には，基底辞書で圧縮したテキストをもう一度圧縮することで得られる再圧縮率を利用する．HOPRDCは新しいハイパーパラメータを使わないため，PRDCと同等の手軽さで利用することができる．
% その結果，判別率は4\%程度向上し，特に色が異なるが形が同じオブジェクトを多く含むクラスで認識精度の向上を確認した．

% NMDのような辞書間距離では，辞書を元データの要約として用いることで，類似度計算の計算量を削減する．その一方で，元データから情報を捨てることが欠点である．
% NMDでは，全単語を長さによらず均等の重みを持つものとして距離計算を実施するため，単語の長さ情報を捨てる．本研究では，単語帳により重みを付け類似度を計算する辞書間距離，WMNDを提案した．その結果，平均適合率は1.6\%程度向上し，中でも単純な領域が多いクラスに対して分類精度が向上した．

NMDのような辞書間距離では，辞書を元データの要約として用いることで，類似度計算の計算量を削減する．その一方で，元データから情報を捨てることが欠点である．
NMDには，辞書中の長い単語が実際のオブジェクト中の長さよりも軽視されるという問題がある．これを解決するために，各単語がその長さによって異なる重みを割り当てられる，新しい辞書間距離であるWNMDを提案する．これによって，WNMDはデータをより忠実に辞書として表現することができる．

実験の結果，HOPRDCおよびWNMDはそれぞれPRDCおよびNMDよりも優れた性能を達成した。

今後の課題として以下のことが挙げられる．
HOPRDCでは，圧縮率と再圧縮率に相関があるため，圧縮率ベクトルの全ての次元を最大限分類に活かすことができていない．マハラノビス距離等を応用し，相関を分類に活かす必要がある．
WNMDでは，より適切な重みを選定することが今後の課題である．
本稿で使用した単語$w$に対する重み$g(w)=\sqrt{|w|}$は，$w$が全て同一文字から構成される場合を想定しているが，実際にはあまりそのような例は無い．
データ中に単語が占める割合をより正確に表す重みを選択することで，精度の向上が期待できる．

% 本稿では，容易に導出できる新しい特徴を利用して，圧縮ベースパターン認識手法の改善を行う．
% 我々は、以前の2つの方法PRDCと最先端のNMDを研究する。
% PRDCに関しては、我々のHOPRDC法は、元のPRDCの単語頻度のみに依存するが、語順に関する情報を特徴ベクトルに埋め込むことに成功する。
% 具体的には、HOPRDCは、圧縮率である再圧縮率を介して単語順の情報を抽出し、再度ベース辞書で圧縮されたファイルを圧縮する。
% 重要なことに、HOPRDCは追加のハイパーパラメータを使用しません。
% NMDに関して、我々は、各単語がその長さに従って異なる重みを割り当てられる新しい辞書距離WNMDを開発する。
% この戦略は、長い単語が実際のオブジェクトよりも辞書の価値が低いという問題を緩和します。
% その結果、WNMDは辞書を元のオブジェクトのより忠実なスケッチとして機能させることができます。
% 実験的に、HOPRDCおよびWNMDはそれぞれPRDCおよびNMDよりも優れた性能を達成する。